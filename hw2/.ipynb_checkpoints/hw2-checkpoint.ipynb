{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/xiaojingu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/xiaojingu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import os\n",
    "import pickle as pkl\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')\n",
    "eng_stopwords = set(stopwords.words('english')).union(set(string.punctuation))\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "\n",
    "def stem_token(token):\n",
    "    \"\"\"\n",
    "        Stem the given token, using any stemmer available from the nltk library\n",
    "        Input: a single token\n",
    "        Output: the stem of the token\n",
    "    \"\"\"\n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "    return PorterStemmer().stem(token)\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "        Tokenize the text.\n",
    "        Input: text - a string\n",
    "        Output: a list of tokens\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def process_text(text):\n",
    "    tokens = []\n",
    "    for token in tokenize(text):\n",
    "        if token.lower() in eng_stopwords:\n",
    "            continue\n",
    "        token = stem_token(token)\n",
    "        token = token.lower()\n",
    "        tokens.append(token)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def read_ap_docs(root_folder=\"./datasets/\"):\n",
    "    dirs = [join(root_folder, \"ap\", \"docs\", 'ap-88'),\n",
    "            join(root_folder, \"ap\", \"docs\", 'ap-89')]\n",
    "    doc_ids = []\n",
    "    docs = []\n",
    "\n",
    "    apfiles = []\n",
    "    for dir in dirs:\n",
    "        apfiles.extend([join(dir, f) for f in listdir(dir) if isfile(\n",
    "            join(dir, f)) and 'ap' in f])\n",
    "\n",
    "    print(\"Reading in documents\")\n",
    "    for apfile in tqdm(apfiles):\n",
    "        with open(apfile, 'r', errors='replace') as reader:\n",
    "            lines = reader.readlines()\n",
    "        line_counter = 0\n",
    "        doc_id = ''\n",
    "        doc = ''\n",
    "        while line_counter < len(lines):\n",
    "            line = lines[line_counter]\n",
    "            if '<DOCNO>' in line:\n",
    "                doc_id = line.split('<DOCNO>')[1].strip().split(\n",
    "                    '</DOCNO>')[0].strip()\n",
    "                doc = ''\n",
    "                doc_ids.append(doc_id)\n",
    "            if '<TEXT>' in line and '</TEXT>' not in line:\n",
    "                line_counter += 1\n",
    "                line = lines[line_counter]\n",
    "                while '</TEXT>' not in line:\n",
    "                    doc += line.strip() + \" \"\n",
    "                    line_counter += 1\n",
    "                    line = lines[line_counter]\n",
    "                if len(docs) == len(doc_ids):\n",
    "                    docs[-1] = doc\n",
    "                else:\n",
    "                    docs.append(doc)\n",
    "                continue\n",
    "            line_counter += 1\n",
    "\n",
    "    return docs, doc_ids\n",
    "\n",
    "\n",
    "def get_processed_docs(doc_set_name=\"processed_docs\"):\n",
    "\n",
    "    path = f\"./{doc_set_name}.pkl\"\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        docs, doc_ids = read_ap_docs()\n",
    "\n",
    "        print(\"Processing documents now\")\n",
    "        doc_repr = {}\n",
    "        p = Pool()\n",
    "        out_p = []\n",
    "        step_size = 1000\n",
    "        start_time = time.time()\n",
    "        for i in range(0, len(docs), step_size):\n",
    "            out_p_local = p.map(\n",
    "                process_text, docs[i:min(len(docs), i+step_size)])\n",
    "            out_p += out_p_local\n",
    "            print(\"Processed %i of %i docs\" % (i+step_size, len(docs)))\n",
    "            time_passed = time.time() - start_time\n",
    "            time_to_go = time_passed * (len(docs)-i-step_size) / (i+step_size)\n",
    "            print(\"Estimated remaining time: %imin %isec\" %\n",
    "                  (int(time_to_go/60.0), int(time_to_go) % 60))\n",
    "\n",
    "        for i in range(len(out_p)):\n",
    "            if len(out_p[i]) > 0:\n",
    "                doc_repr[doc_ids[i]] = out_p[i]\n",
    "\n",
    "        with open(path, \"wb\") as writer:\n",
    "            pkl.dump(doc_repr, writer)\n",
    "\n",
    "        print(f\"all docs processed. saved to {path}\")\n",
    "\n",
    "        return doc_repr\n",
    "    else:\n",
    "        print(\"Docs already processed. Loading from disk\")\n",
    "\n",
    "        with open(path, \"rb\") as reader:\n",
    "            return pkl.load(reader)\n",
    "\n",
    "\n",
    "def read_qrels(root_folder=\"./datasets/\"):\n",
    "\n",
    "    qrels = {}\n",
    "    queries = {}\n",
    "\n",
    "    with open(os.path.join(root_folder, \"ap\", \"qrels.tsv\")) as reader:\n",
    "        for line in reader:\n",
    "            qid, _, doc_id, _ = line.split(\"\\t\")\n",
    "            if qid not in qrels:\n",
    "                qrels[qid] = {}\n",
    "            qrels[qid][doc_id] = 1\n",
    "\n",
    "    with open(os.path.join(root_folder, \"ap\", \"queries.tsv\")) as reader:\n",
    "        for line in reader:\n",
    "            qid, query = line.split(\"\\t\")\n",
    "            if qid in qrels:\n",
    "                queries[qid] = query\n",
    "\n",
    "    return qrels, queries\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs already processed. Loading from disk\n",
      "164557\n"
     ]
    }
   ],
   "source": [
    "import read_ap\n",
    "from gensim.test.utils import common_dictionary, common_corpus\n",
    "from gensim.models import LsiModel\n",
    "\n",
    "model = LsiModel(common_corpus, id2word=common_dictionary)\n",
    "vectorized_corpus = model[common_corpus]\n",
    "\n",
    "docs = read_ap.get_processed_docs()\n",
    "\n",
    "print(len(docs))\n",
    "\n",
    "pass\n",
    "\"\"\"\n",
    "use gensim to get a vocabulary from the processed docs => Dictionary\n",
    "\n",
    "pass to gensim's bow or tfidf functions => corpus\n",
    "\n",
    "pass corpus to gensim models\n",
    "\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46270240"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "keys = list(docs.keys())#[:100]\n",
    "count = 0\n",
    "for key in keys:\n",
    "    count += len(docs[key])\n",
    "#     print(key, docs[key])\n",
    "#     print()\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164557\n"
     ]
    }
   ],
   "source": [
    "# model = LsiModel(docs)\n",
    "# vectorized_corpus = model[common_corpus]\n",
    "\n",
    "# docs = read_ap.get_processed_docs()\n",
    "\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequent_tokens(docs, doc_ids, min_threshold=50):\n",
    "    corpus_token_count = Counter()\n",
    "\n",
    "    doc_ids = doc_ids[:500]\n",
    "    \n",
    "    for doc_id in tqdm(doc_ids):\n",
    "        corpus_token_count = corpus_token_count + Counter(docs[doc_id])\n",
    "\n",
    "    # filter out unfrequent words\n",
    "    thresholded_token_counts = Counter(el for el in corpus_token_count.elements() if corpus_token_count[el] >= min_threshold)\n",
    "\n",
    "    return set(thresholded_token_counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/500 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█▏        | 61/500 [00:00<00:00, 599.09it/s]\u001b[A\n",
      " 18%|█▊        | 90/500 [00:00<00:00, 452.00it/s]\u001b[A\n",
      " 24%|██▍       | 122/500 [00:00<00:00, 401.93it/s]\u001b[A\n",
      " 30%|███       | 152/500 [00:00<00:00, 360.71it/s]\u001b[A\n",
      " 37%|███▋      | 183/500 [00:00<00:00, 341.42it/s]\u001b[A\n",
      " 42%|████▏     | 211/500 [00:00<00:00, 301.31it/s]\u001b[A\n",
      " 48%|████▊     | 238/500 [00:00<00:00, 263.66it/s]\u001b[A\n",
      " 53%|█████▎    | 263/500 [00:00<00:00, 248.49it/s]\u001b[A\n",
      " 57%|█████▋    | 287/500 [00:00<00:00, 245.37it/s]\u001b[A\n",
      " 62%|██████▏   | 311/500 [00:01<00:00, 236.86it/s]\u001b[A\n",
      " 67%|██████▋   | 335/500 [00:01<00:00, 229.23it/s]\u001b[A\n",
      " 72%|███████▏  | 358/500 [00:01<00:00, 214.96it/s]\u001b[A\n",
      " 76%|███████▌  | 380/500 [00:01<00:00, 213.38it/s]\u001b[A\n",
      " 80%|████████  | 402/500 [00:01<00:00, 208.50it/s]\u001b[A\n",
      " 85%|████████▍ | 423/500 [00:01<00:00, 202.27it/s]\u001b[A\n",
      " 89%|████████▉ | 444/500 [00:01<00:00, 195.09it/s]\u001b[A\n",
      " 93%|█████████▎| 464/500 [00:01<00:00, 190.55it/s]\u001b[A\n",
      "100%|██████████| 500/500 [00:02<00:00, 239.73it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "freq_docs = get_frequent_tokens(docs, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( freq_docs)\n",
    "\n",
    "s = set()\n",
    "\n",
    "type(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17020253"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter = 0\n",
    "# create docs corpus matrix\n",
    "docs_matrix = []\n",
    "\n",
    "for key in keys:\n",
    "    \n",
    "    counter += len(set(docs[key]) - freq_docs)\n",
    "    \n",
    "    \n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gensim dictionaries\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = Dictionary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
